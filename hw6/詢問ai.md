線性迴歸問題 甚麼是爬山演算法 貪婪法 改良法 梯度下降法

這是一個非常好的問題。在討論線性迴歸（Linear Regression）時，我們通常關注的是如何找到一條「最佳擬合線」，也就是找到一組參數（權重 w 和偏差 b），讓預測值與實際值之間的誤差（Loss）最小化。

你提到的這四個概念，其實代表了尋找這個最小誤差過程中的不同策略與演進。我們可以把它想像成一個「尋找山谷最低點（最小誤差）」的過程。

以下我將為你詳細拆解這些概念，並說明它們在優化問題中的角色。

1. 爬山演算法 (Hill Climbing)
核心概念：只看眼前，盲目探索

爬山演算法是一種最直觀的搜尋策略。想像你在濃霧中想要下山（尋找誤差最低點），但你看不見全貌。

做法： 你隨機選擇一個起點，然後向前後左右踏一步試探。

決策： 如果往某個方向走一步，高度降低了（誤差變小），你就真的往那裡走一步；如果高度變高了，你就留在原地或換個方向試。

線性迴歸中的應用： 隨機調整權重 w，如果誤差變小就保留新的 w，否則放棄。

缺點： 容易陷入局部最佳解 (Local Optima)。就像你走到了一個小山坳，四週都比你高，你以為到了最低點（谷底），但其實真正的谷底還在更遠的地方。但在簡單的線性迴歸（通常是凸函數）中，這個問題較小，主要問題是效率極低。

2. 貪婪法 (Greedy Method)
核心概念：每一步都選當下最好的

貪婪法是一種在每一步選擇中都採取在當前狀態下最好或最優（即最有利）的選擇，希望導致結果是全域最好或最優的演算法。

線性迴歸中的應用： 在單純求解 w 和 b 時較少直接稱為「貪婪法」。但在特徵選擇 (Feature Selection) 中非常常見，稱為逐步迴歸 (Stepwise Regression)：

面對 10 個變數，先選出 1 個跟結果相關性最高的（當下最好的）。

固定第一個，再從剩下 9 個中選出一個加入後能讓誤差降最多的。

以此類推。

缺點： 短視近利。雖然每一步都選最好的，但組合起來未必是整體的最佳解。

3. 改良法 (Improvement Methods)
核心概念：為了克服爬山法的缺陷

這通常是指為了修正爬山演算法容易卡在「局部最佳解」而衍生出的技術。既然爬山法一遇到四周都變高就停止，改良法就是允許「例外」。

常見的改良策略包括：

隨機重啟 (Random Restart)： 如果卡住了，就隨機換一個地方重新開始爬，最後比較哪一次的結果最好。

模擬退火 (Simulated Annealing)： 模仿金屬冷卻的過程。剛開始（溫度高時），允許偶爾往「高處」走（接受較差的解），這樣有機會跳出小山坳；隨著時間過去（溫度冷卻），就變得越來越嚴格，只接受往低處走。

4. 梯度下降法 (Gradient Descent)
核心概念：利用數學導航，快速下山

這是現代機器學習和線性迴歸中最主流、最強大的方法。它不是盲目試探，而是利用微積分。

做法： 計算損失函數（Loss Function）對參數的微分（梯度）。梯度就是「斜率」，它告訴我們：

方向： 哪邊是下坡（梯度的反方向）。

陡峭程度： 坡度越陡，步步伐可以跨大一點；坡度平緩，步伐就小一點。

公式概念：

w 
new
​
 =w 
old
​
 −Learning Rate×Gradient
（新的位置 = 舊位置 - 步伐 × 斜率）

優點： 效率遠高於爬山法。因為它知道確切的「下坡方向」，而不需要四處盲目亂試。對於線性迴歸這種「碗狀」（凸函數）圖形，梯度下降法保證能滑到最底端（全域最佳解）。
